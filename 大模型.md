作者：Rooters
链接：https://zhuanlan.zhihu.com/p/657826357
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

一些比较高频的东西（针对基座算法/框架岗位为主，大体按重要性排序）：
* 1.多头注意力，频率太高了。coding轮，概念轮都考。复习的点包括：时间/空间复杂度，优化（kv-cache，MQA，GQA），手写多头代码。各种Norm，这个频率也不低，不过比较标准的内容，没有啥特意要说的，有的考手写，有的考概念和理解（为什么管用）。
* 2. 框架相关内容，各种并行方式，优缺点。DeepSpeed，Megatron可以看看源代码，Flash-Attention等内容。这个点也经常考代码题。
*  3.  BERT，GPT等比较主流大模型，一些细节，比如位置编码，训练loss，激活，架构些许不同这种。自回归重点。
*  4.  大模型训练，这个可能主要是工作经验相关，经常问比如训练loss炸掉了，如何解决，一些技巧之类的。面试时有些面试官会问一些很细节的东西，感觉是在确认确实上手跑过基座训练不是吹水。
*  5.  数据预处理，BPE，tokenization，mask相关概念和对模型/训练影响，数据配比（有paper）。
*  6.  evaluation，如何评估大模型，安全性，有效性，公开数据，个别考过手写eval框架（多选，生成）。
*  7. 根据投的岗位，多模态和RLHF内容可以适当看看。这俩感觉paper挺重要的，也大多研究岗位。楼主也少面了一些自动驾驶，RL啥的，不过结果不咋地。
*  PS：有些非基座相关的或者实在没啥印象的就没写。还有几家背景和面试感觉特别好的公司，不过猎头特别说了要保密，不方便发，如果有朋友想了解的话可私信。


# KV-Cache 是指在多头注意力机制中，存储之前计算好的 Key (K) 和 Value (V) 的值，以避免在推理长序列时重复计算从而提升效率


# 为什么训练阶段通常不使用 KV-Cache？
1.1. 全序列计算（也可以刚开始模型不准确时候全序列计算，后面开始用增量计算）
在训练阶段，模型通常以完整的序列为输入，并进行并行化计算。对于每个 token，都会计算从输入序列中的所有 token 到该 token 的注意力，这种操作是全并行的。
如果使用 KV-Cache，就需要模拟逐步生成过程，这种方法将引入序列化（sequential processing），大幅降低训练效率。
1.2. 动态性与梯度计算
训练时需要进行反向传播来更新模型参数。
如果使用 KV-Cache，需要在每一步保存 Key 和 Value 的中间状态，同时这些缓存还需要支持梯度计算，会显著增加显存需求和计算复杂度。
1.3. 高效并行优化
训练阶段通常使用 矩阵操作的高度并行化 优化硬件利用率。例如，Transformer 的 Attention 是对整个输入序列一次性计算完成的，能最大化 GPU 的计算能力。
KV-Cache 是一种增量更新机制，在生成序列时有效，但在训练中无法充分利用矩阵计算的并行特性。

# 
