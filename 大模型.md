作者：Rooters
链接：https://zhuanlan.zhihu.com/p/657826357
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

一些比较高频的东西（针对基座算法/框架岗位为主，大体按重要性排序）：
* 1.多头注意力，频率太高了。coding轮，概念轮都考。复习的点包括：时间/空间复杂度，优化（kv-cache，MQA，GQA），手写多头代码。各种Norm，这个频率也不低，不过比较标准的内容，没有啥特意要说的，有的考手写，有的考概念和理解（为什么管用）。
* 2. 框架相关内容，各种并行方式，优缺点。DeepSpeed，Megatron 可以看看源代码，Flash-Attention等内容。这个点也经常考代码题。
*  3.  BERT，GPT等比较主流大模型，一些细节，比如位置编码，训练loss，激活，架构些许不同这种。自回归重点。
*  4.  大模型训练，这个可能主要是工作经验相关，经常问比如训练loss炸掉了，如何解决，一些技巧之类的。面试时有些面试官会问一些很细节的东西，感觉是在确认确实上手跑过基座训练不是吹水。
*  5.  数据预处理，BPE，tokenization，mask相关概念和对模型/训练影响，数据配比（有paper）。
*  6.  evaluation，如何评估大模型，安全性，有效性，公开数据，个别考过手写eval框架（多选，生成）。
*  7. 根据投的岗位，多模态和RLHF内容可以适当看看。这俩感觉paper挺重要的，也大多研究岗位。楼主也少面了一些自动驾驶，RL啥的，不过结果不咋地。
*  PS：有些非基座相关的或者实在没啥印象的就没写。还有几家背景和面试感觉特别好的公司，不过猎头特别说了要保密，不方便发，如果有朋友想了解的话可私信。


# KV-Cache 是指在多头注意力机制中，存储之前计算好的 Key (K) 和 Value (V) 的值，以避免在推理长序列时重复计算从而提升效率


# 为什么训练阶段通常不使用 KV-Cache？
1.1. 全序列计算（也可以刚开始模型不准确时候全序列计算，后面开始用增量计算）
在训练阶段，模型通常以完整的序列为输入，并进行并行化计算。对于每个 token，都会计算从输入序列中的所有 token 到该 token 的注意力，这种操作是全并行的。
如果使用 KV-Cache，就需要模拟逐步生成过程，这种方法将引入序列化（sequential processing），大幅降低训练效率。
1.2. 动态性与梯度计算
训练时需要进行反向传播来更新模型参数。
如果使用 KV-Cache，需要在每一步保存 Key 和 Value 的中间状态，同时这些缓存还需要支持梯度计算，会显著增加显存需求和计算复杂度。
1.3. 高效并行优化
训练阶段通常使用 矩阵操作的高度并行化 优化硬件利用率。例如，Transformer 的 Attention 是对整个输入序列一次性计算完成的，能最大化 GPU 的计算能力。
KV-Cache 是一种增量更新机制，在生成序列时有效，但在训练中无法充分利用矩阵计算的并行特性。

# 为什么token emb 可以和 position emb相加
计算dot-product后，可以分解为3项相加： token-token表示相似度， position-position表示位置， token-position 没有用。

# Multi-Query Attention 的核心思想是：多个查询头共享 Key 和 Value，但每个头仍保持独立的 Query。这种共享机制降低了计算和存储需求

# GQA 的核心思想在 GQA 中，多个查询头共享 Key 和 Value，同时将 Query 按组共享

# 各种norm
1. pre LN 和 post LN：https://kexue.fm/archives/9009
2. 目前比较明确的结论是：同一设置之下，Pre Norm结构往往更容易训练，但最终效果通常不如Post Norm。
3. RMS Normalization (RMSNorm)， 原理：类似于 LN，但只对激活值的均方根（RMS）进行归一化，而不减去均值
Transformer 模型：大多数采用 LayerNorm 或 RMSNorm。
推理效率优先：可考虑 RMSNorm 或 ScaleNorm。
小批次训练：建议使用 GroupNorm。
实验性研究或优化器改进：可尝试 NFNets 或 WeightNorm。

# Flash-Attention 
FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
1. 减少io访问量
2. 分块计算，融合多个操作，减少中间结果缓存
3. 计算少的结果计算梯度时候重新计算，不缓存
4. 

# 分布式训练
1. data parall： tensorflow的ps-worker架构
2. distribute data parall:
   * ring all-reduce, gpu 连成一个环，每个gpu接受左边的梯度，累加自己的，然后传给右边。
   * 其次，为了均衡带宽（不把参数聚合在某一台机器），整个模型被分成不同部分，每个部分有一个环。
   * 和集群大小无关
3. DeepSpeed zero： 零冗余
   * 原理，优化器是显存的大头，gpu之间保留整个模型的优化器是很浪费的
   * 把adam的一阶，二阶动量分机器储存
4. Megatron-LM 是训练超大规模 Transformer 语言模型的强大工具
   * 模型并行：张量并行，把tensor切分了
5. gpipe
   * GPIPE 是一种高效的流水线并行（Pipeline Parallelism）方法
   * 模型并行
  
# 位置编码
1. 绝对位置编码（其实也包含相对位置信息）
   * 三角函数式
   * 可学习是
3. 相对位置编码
4. 旋转位置编码：rope
   * 觉得光有相对位置也不行，还是想要绝对位置信息
   * 先设定一个目标：构造一个token emb中包含绝对位置信息，同时内积又是 m-n 相对位置的函数（注意这里的减法，比三角函数能更好的刻画相对位置）。然后反推去找函数。
   * 最后实现是给token emb左乘了一个旋转矩阵
   * 它的主要改动是应用了笔者构思的“旋转式位置编码（Rotary Position Embedding，RoPE）”，这是一种配合Attention机制能达到“绝对位置编码的方式实现相对位置编码”的设计。而也正因为这种设计，它还是目前唯一一种可用于线性Attention的相对位置编码。
   * 
     
https://www.bilibili.com/video/BV1xR1RY9ECm/?spm_id_from=333.337.search-card.all.click&vd_source=e0660eff63933669bb700b01739a036b
<img width="829" alt="image" src="https://github.com/user-attachments/assets/fca78915-43bf-4635-a81a-6bb5e3d881e4" />

